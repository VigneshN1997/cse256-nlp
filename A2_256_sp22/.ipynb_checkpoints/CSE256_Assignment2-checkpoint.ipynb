{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQxvO6dhMvtk"
   },
   "source": [
    "# CSE 256: Statistical NLP UCSD Assignment 2\n",
    "## Exploring Word Vectors (12.5 points + 1 bonus point)\n",
    "## Vignesh Nanda Kumar PID: A59010704\n",
    "### <font color='blue'> Due 11:59pm, Monday April 18, 2022 </font>\n",
    "\n",
    "\n",
    "Before you start, make sure you read the README.txt in the same directory as this notebook.\n",
    "\n",
    "\n",
    "**Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2041,
     "status": "ok",
     "timestamp": 1649549290401,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "kCkBxLqTMvtn",
    "outputId": "bfe197d9-3b2f-4d56-d77d-466d8e3ec722"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/vignesh/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not add to this list.\n",
    "# ----------------\n",
    "\n",
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4q8E45iMvto"
   },
   "source": [
    "## Word Vectors\n",
    "\n",
    "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore word vectors derived from *Word2Vec*. \n",
    "\n",
    "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding) states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fO3hIO8IMvtw"
   },
   "source": [
    "## Word Vectors \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We shall explore the embeddings produced by word2vec. Please revisit the class notes and lecture slides for more details on the word2vec algorithm. Paper 1 review due May 4th, involves reading the  [word2vec  paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf),  reading it now might help you with this assignment.\n",
    "\n",
    "Run the following cells to load the word2vec vectors into memory. **Note**: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes. In *Colab*, the embeddings are downloaded to the server everytime you restart the notebook). For this reason, you may prefer to work on your local machine where the download only happens once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1649549350997,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "bFrL7EjHMvtw"
   },
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load Word2Vec Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All the embeddings\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_from_bin = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    print(\"Loaded vocab size %i\" % len(wv_from_bin.vocab.keys()))\n",
    "    return wv_from_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 797891,
     "status": "ok",
     "timestamp": 1649550154639,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "aSWjn1UvMvtw",
    "outputId": "6169993d-2930-4fa4-931d-85d07008d4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size 3000000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Run Cell to Load Word Vectors\n",
    "# Note: This will take a couple minutes\n",
    "# -----------------------------------\n",
    "wv_from_bin = load_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnhwd5uuMvtw"
   },
   "source": [
    "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1sLJ47CMvtw"
   },
   "source": [
    "\n",
    "### Plot function\n",
    "Let's define a plot function that reduces the vectors from 300-dimensions to 2-dimensions, and visualises them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1649550966366,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "gI86TKrUMvtw"
   },
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.key_to_index.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQpvbQDFMvtx"
   },
   "source": [
    "### Question 1: Word2vec Plot Analysis [written] (2 points)\n",
    "\n",
    "Run the cell below to plot the 2D GloVe embeddings for `['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']`.\n",
    "\n",
    "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1649550972955,
     "user": {
      "displayName": "Ndapandula Nakashole",
      "userId": "12501637879061685337"
     },
     "user_tz": 420
    },
    "id": "hV6LXSCEMvtx",
    "outputId": "340f76a3-70e8-41fb-ea77-6ac0a7ba388a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
    "display_pca_scatterplot(wv_from_bin, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w7xtcUSMvtx"
   },
   "source": [
    "#### <font color=\"red\">Oil, petroleum, output is one cluster,  energy and  industry is another cluster relating to the topic of fuel industry. Barrels, bpd also cluster together. Kuwait, Equador, Iraq doesn't cluster together but should've because all of them are countries. Also Iraq and Kuwait could've been more closer to oil given that they are major oil producing nations so there could be sentences which have these countries occuring in context of oil. Also, barrels and bpd (barrels per day could've been closer to petroleum since they also relate to the petroleum industry. But it could be possible that barrels was used in a context other than the petroleum industry and hence is not closer to petroleum.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ny56x_fMvtx"
   },
   "source": [
    "### Cosine Similarity\n",
    "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
    "\n",
    "The [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEWzAas1Mvtx"
   },
   "source": [
    "### Question 2: Words with Multiple Meanings (2 points) [code + written] \n",
    "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"rock\" has both \"music\" and \"stone\". You will probably need to try several polysemous or homonymic words before you find one. \n",
    "\n",
    "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
    "\n",
    "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4kTQ066WMvtx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crappie', 0.6551623344421387),\n",
       " ('largemouth', 0.6519780158996582),\n",
       " ('largemouths', 0.6363433599472046),\n",
       " ('largemouth_bass', 0.6293675899505615),\n",
       " ('striper', 0.6191805601119995),\n",
       " ('stripers', 0.6170703768730164),\n",
       " ('smallmouth', 0.6161339282989502),\n",
       " ('Spotted_bass', 0.6154202222824097),\n",
       " ('acoustic_bass', 0.613075852394104),\n",
       " ('upright_bass', 0.6106366515159607)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    word = \"bass\"\n",
    "    wv_from_bin.most_similar(word)\n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D9rhZJc1psV"
   },
   "source": [
    "#### <font color=\"red\">The word is bass which is a type of fish and also a type of singing voice. The top 10 related words contains words from both meanings: fish (largemouth_bass, smallmouth are types of bass fish), and acoustic bass (related to music and also a type of guitar). Many of the words didn't work because it could be possible that the corpus on which the model was trained did not contain the word used context of its second meaning. Or the word occurred in context of its first meaning much more and in context of its second meaning much less (so the first meaning related words are closer in cosine distance and come in top 10 of the word, while the second meaning related words would be far off) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVll_55fMvtx"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2KRcfnVMvty"
   },
   "source": [
    "### Question 3: Analogies with Word Vectors [written] (2 points)\n",
    "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies. \n",
    "\n",
    "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list (while omitting the input words, which are often the most similar; see [this paper](https://www.aclweb.org/anthology/N18-2039.pdf)). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YysbnlIWMvty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071),\n",
      " ('monarch', 0.6189674139022827),\n",
      " ('princess', 0.5902431011199951),\n",
      " ('crown_prince', 0.5499460697174072),\n",
      " ('prince', 0.5377321243286133),\n",
      " ('kings', 0.5236844420433044),\n",
      " ('Queen_Consort', 0.5235945582389832),\n",
      " ('queens', 0.518113374710083),\n",
      " ('sultan', 0.5098593235015869),\n",
      " ('monarchy', 0.5087411999702454)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to answer the analogy -- man : king :: woman : x\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOezwAj0Mvty"
   },
   "source": [
    "Let $m$, $k$, $w$, and $x$ denote the word vectors for `man`, `king`, `woman`, and the answer, respectively. Using **only** vectors $m$, $k$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, what is the expression in which we are maximizing cosine similarity with $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeJ07PABMvty"
   },
   "source": [
    "#### <font color=\"red\">x = (k - m + w)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaHnrgCeMvty"
   },
   "source": [
    "### Question 4: Finding Analogies [code + written]  (1 point)\n",
    "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy you came up might not be obvious to the TAs, explain why the analogy holds in one or two sentences.\n",
    "\n",
    "**Note**: You may have to try many analogies to find one that works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EpcYBn5yMvty"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tokyo', 0.8142861127853394),\n",
      " ('Toyko', 0.659669816493988),\n",
      " ('Osaka', 0.6350962519645691),\n",
      " ('Nagoya', 0.6258591413497925),\n",
      " ('Seoul', 0.6054927706718445),\n",
      " ('Japanese', 0.5919331312179565),\n",
      " ('Yokohama', 0.5900902152061462),\n",
      " ('Osaka_Japan', 0.585975170135498),\n",
      " ('Takamatsu', 0.57918381690979),\n",
      " ('Fukuoka', 0.5664029121398926)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pprint.pprint(wv_from_bin.most_similar(positive=['Japan', 'Paris'], negative=['France']))\n",
    "    \n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhwe3RO-Mvty"
   },
   "source": [
    "#### <font color=\"red\">France:Paris :: Japan:Tokyo . The analogy is of country : capital</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se6pOUqQMvtz"
   },
   "source": [
    "### Question 5: Incorrect Analogy [code + written] (2 point)\n",
    "Find an example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nuZg40J8Mvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cars', 0.4880220890045166),\n",
      " ('Car', 0.46311694383621216),\n",
      " ('Mazda_MX5', 0.46152472496032715),\n",
      " ('SUV', 0.4557592272758484),\n",
      " ('Volkswagon_Polo', 0.4556606709957123),\n",
      " ('Ford_Focus', 0.44932806491851807),\n",
      " ('Honda_Civic', 0.44522327184677124),\n",
      " ('motorbike', 0.443489134311676),\n",
      " ('vehicle', 0.44239693880081177),\n",
      " ('minivan', 0.43566223978996277)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pprint.pprint(wv_from_bin.most_similar(positive=['car', 'sea'], negative=['ship']))\n",
    "\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvmrmj2oMvtz"
   },
   "source": [
    "#### <font color=\"red\">ship:sea :: car:road is the intended analogy (vehicle:where the vehicle moves). The incorrect value is \"cars\".</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDyys-8DMvtz"
   },
   "source": [
    "### Question 6: Guided Analysis of Bias in Word Vectors [written] (2 point)\n",
    "\n",
    "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
    "\n",
    "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mTyRfo1DMvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('workers', 0.6582455635070801),\n",
      " ('employee', 0.5805293321609497),\n",
      " ('nurse', 0.5249921679496765),\n",
      " ('receptionist', 0.5142490267753601),\n",
      " ('migrant_worker', 0.5001609325408936),\n",
      " ('Worker', 0.4979269802570343),\n",
      " ('housewife', 0.48609834909439087),\n",
      " ('registered_nurse', 0.4846190810203552),\n",
      " ('laborer', 0.48437267541885376),\n",
      " ('coworker', 0.48212406039237976)]\n",
      "\n",
      "[('workers', 0.5590360164642334),\n",
      " ('laborer', 0.54481041431427),\n",
      " ('foreman', 0.5192232131958008),\n",
      " ('Worker', 0.5161596536636353),\n",
      " ('employee', 0.5094279050827026),\n",
      " ('electrician', 0.49481213092803955),\n",
      " ('janitor', 0.48718899488449097),\n",
      " ('bricklayer', 0.4825313091278076),\n",
      " ('carpenter', 0.47498998045921326),\n",
      " ('workman', 0.4642517566680908)]\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n",
    "# most dissimilar from.\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
    "print()\n",
    "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1t37qzjMvtz"
   },
   "source": [
    "#### <font color=\"red\">The female associated words are nurse, receptionist, housewife, coworker while the male associated words are foreman, electrician, janitor, carpenter which shows the extreme gender stereotype being brought out by the model. For example for female the related word is coworker but for male foreman is a related word. This is reflecting gender bias that only males can supervise and direct other workers. Also, occupations like nurse, receptionist are coming in female related words while electrician, carpenter are coming in male related words. These occupations should be gender neutral but instead the model is reflecting gender bias through these related words for man and woman.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI5PRZVYMvtz"
   },
   "source": [
    "### Question 7: Independent Analysis of Bias in Word Vectors [code + written]  ( 1.5 point)\n",
    "\n",
    "Use the `most_similar` function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fW5_tJNeMvtz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('workers', 0.555210292339325),\n",
      " ('laborer', 0.48098012804985046),\n",
      " ('migrant_worker', 0.45900607109069824),\n",
      " ('employee', 0.45884913206100464),\n",
      " ('Worker', 0.44189125299453735),\n",
      " ('housemaid', 0.42111337184906006),\n",
      " ('mineworker', 0.4171217978000641),\n",
      " ('Makhosi', 0.41370075941085815),\n",
      " ('Bogopane_Zulu', 0.41295868158340454),\n",
      " ('housekeeper', 0.41201385855674744)]\n",
      "[('workers', 0.5497925877571106),\n",
      " ('employee', 0.5275042057037354),\n",
      " ('Worker', 0.4733121693134308),\n",
      " ('laborer', 0.4481498897075653),\n",
      " ('technician', 0.4288828372955322),\n",
      " ('janitor', 0.4153149724006653),\n",
      " ('forklift_operator', 0.40671056509017944),\n",
      " ('electrician', 0.406697541475296),\n",
      " ('supervisor', 0.40284863114356995),\n",
      " ('em_ployee', 0.39987462759017944)]\n"
     ]
    }
   ],
   "source": [
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    # pprint.pprint(wv_from_bin.most_similar(positive=['raj', 'white'], negative=['vincent']))\n",
    "    pprint.pprint(wv_from_bin.most_similar(positive=['african', 'worker'], negative=['american']))\n",
    "    pprint.pprint(wv_from_bin.most_similar(positive=['american', 'worker'], negative=['african']))\n",
    "    # ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArSm9i1rMvt0"
   },
   "source": [
    "#### <font color=\"red\">The analogy brings out the racial bias or occupational bias present in word embeddings. For Africans, the first set has words like laborer, migrant worker, housemaid, mine worker, house keeper. For american the related words are employee, laborer, technician, janitor, operator, electrician, supervisor.  Here the racial bias can be clearly seen between the occupation results for both the communities. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL9C0xPyMvt0"
   },
   "source": [
    "### Question 8: Thinking About Bias [written] (Bonus: 1 point)\n",
    "\n",
    "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSbxt9EYMvt0"
   },
   "source": [
    "#### <font color=\"red\"> Bias can get into the model from the training data. The word embeddings are generated from the given text data, so if the data is biased towards a particular gender or race or religion or any other type of bias, then the word embedding model will also be biased.(Discussed the experiment with another student)An experiment that can be done to measure the source of bias can be first find out the different groups pertaining to a particular type of bias. For example lets say we take up different religions as groups. Now we can use the intrinsic evaluation technique of analogies to see the distance between 2 religion words (R1, R2) and a common word C. The distance between R1, C should be similar to the distance between R2 and C. The difference between the distances can be used to evaluate the bias present in the model. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx4fdrswMvt0"
   },
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells). \n",
    "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "3. Once you've rerun everything, select File -> Download as -> PDF via LaTeX (If you have trouble using \"PDF via LaTex\", you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions especially the coding parts are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
    "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "5. Submit your PDF on Gradescope.\n",
    "\n",
    "\n",
    "#### <font color=\"blue\"> Acknowledgements</font>\n",
    "This assignment is based on an assignment developed by Chris Manning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "cnhwd5uuMvtw",
    "0w7xtcUSMvtx",
    "9Ny56x_fMvtx",
    "xeJ07PABMvty",
    "Fhwe3RO-Mvty",
    "jvmrmj2oMvtz",
    "N1t37qzjMvtz",
    "ArSm9i1rMvt0",
    "bL9C0xPyMvt0",
    "CSbxt9EYMvt0"
   ],
   "name": "CSE256_Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
